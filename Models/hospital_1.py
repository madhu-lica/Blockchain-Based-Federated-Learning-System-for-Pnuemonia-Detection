# -*- coding: utf-8 -*-
"""Hospital_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1acMFrplwS4BPj2iXynS0HPXLg6mimdgL

# 1.  Formulating our objective.
**AIM** to **classify the images of X-rays** to **identify** which show signs of **pneumonia**. It is a **binary classfication**.
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams['figure.figsize']=(20,20)

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

zip_ref = zipfile.ZipFile('/content/drive/MyDrive/H1_data.zip', 'r') #Opens the zip file in read mode
zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder
zip_ref.close()

"""# 2. Looking at our images.

**Pnuemonia X Ray Image**
"""

from glob import glob #retriving an array of files in directories
path_train = "/tmp/train"
img = glob(path_train+"/PNEUMONIA/*.jpeg")
# print(img)
img = np.asarray(plt.imread(img[0]))
plt.figure(figsize = (5 , 5))
plt.imshow(img)

"""**Normal X Ray Image**"""

img = glob(path_train+"/NORMAL/*.jpeg")
img = np.asarray(plt.imread(img[0]))
plt.figure(figsize = (5 , 5))
plt.imshow(img)

"""# 3. Acquiring our dataset from the directories.

**Importing libraries that are used for creating a convolution neural network model.**
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array

from tensorflow.keras.models import Sequential,Model

from tensorflow.keras.layers import Conv2D,Dense,Flatten,Input,MaxPooling2D,Dropout,BatchNormalization

"""**Using Image Data Generator we load the images to our model.**

**What kind of data augmentation techniques will you use that are suitable for this problem ?**
* Horizontal Flip : a data augmentation technique that takes both rows and columns of such a matrix and flips. them horizontally. 
* Brightness Range : increases the overall lightness of the image.
* Height and Width Shift
* Rotation of the Image :we can rotate the image by 0 to 360 degrees clockwise.
"""

img_size=150
batch_size=25

traingen=ImageDataGenerator(rescale=1/255.,
                           rotation_range=50,
                        brightness_range=[0.2,1.2],
                           width_shift_range=0.1,
                           height_shift_range=0.1,
                           horizontal_flip=True)


testgen=ImageDataGenerator(rescale=1/255.)

valgen=ImageDataGenerator(rescale=1/255.)

"""* **traindata** : stores all the images from the train directroy
* **testdata** : stores all the images from the test directory
* **valdata** : stores all the images from the val directory
"""

traindata=traingen.flow_from_directory('/tmp/train',
                                       target_size=(img_size,img_size)
                                       ,batch_size=batch_size,
                                       shuffle=True,class_mode='binary'
                                      ,color_mode='grayscale')

testdata=testgen.flow_from_directory('/tmp/test',
                                    shuffle=False,batch_size=batch_size,
                                    target_size=(img_size,img_size),
                                    class_mode="binary",color_mode='grayscale')
valdata=valgen.flow_from_directory('/tmp/val',
                                    shuffle=False,batch_size=batch_size,
                                    target_size=(img_size,img_size),
                                    class_mode="binary",color_mode='grayscale')



"""**Printing 15 images from the train data**"""

import scipy
labels=['Normal','Pnuemonia']
samples=traindata.__next__()

images=samples[0]
target=samples[1]

for i in range(15):
    plt.subplot(5,5,i+1)
    plt.subplots_adjust(hspace=0.3,wspace=.3)
    plt.imshow(images[i])
    plt.title(f"Class: {labels[int(target[i])]}")
    plt.axis('off')

"""**Meta Data**

0 here is a label Normal and 1 is for Pnuemonia
"""

df=pd.DataFrame(traindata.classes)
df.value_counts()

df=pd.DataFrame(valdata.classes)
df.value_counts()

df=pd.DataFrame(testdata.classes)
df.value_counts()

"""# 4. Creating our Counvolution Neural Network."""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
model=Sequential()
model.add(Conv2D(32,(2,2),input_shape=(img_size,img_size,1),activation="relu",padding='same',strides=1))
model.add(MaxPooling2D())
model.add(Conv2D(64,(2,2),strides=2,activation="relu",padding='same'))
model.add(MaxPooling2D())
model.add(Conv2D(128,(2,2),strides=1,activation="relu",padding='same'))
model.add(MaxPooling2D())
model.add(Flatten())
model.add(Dense(128,activation="relu"))
model.add(Dense(1,activation='sigmoid'))

model.summary()

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

"""* **ModelCheckpoint** callback is used in conjunction with training using model. fit() to save a model or weights in a checkpoint file at some time, so the model or weights can be loaded later to continue the training from the state saved.
* **Early stopping** is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset

"""

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
filepath= "H1.h5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)
# es = EarlyStopping(monitor='accuracy', patience=5)

"""# 5. Training Our Convolution Neural Network."""

history=model.fit(traindata,validation_data=valdata,epochs=10,callbacks=[checkpoint])

"""**Plotting Lossand and Accuracy Curves from training and validation**"""

plt.figure(figsize=(20,8))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

plt.figure(figsize=(20,8))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

"""## Checking the model prediction for the val data"""

y_test = valdata.classes
y_pred = model.predict(valdata)
y_pred_probs = y_pred.copy()
y_pred[y_pred>0.5] = 1
y_pred[y_pred<0.5] = 0

from sklearn.metrics import classification_report, confusion_matrix

"""**Classification Report**"""

print(classification_report(y_test,y_pred,target_names = ['Normal','Pnuemonia']))

"""**Confusion Matrix**"""

plt.figure(figsize=(10,8))
sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt='.3g',xticklabels=['Normal','Pnuemonia'],
            yticklabels=['Normal','Pnuemonia'],cmap='Blues')
plt.show()

"""## Checking the model prediction for the test data"""

y_test = testdata.classes
y_pred = model.predict(testdata)
y_pred_probs = y_pred.copy()

y_pred[y_pred>0.5] = 1
y_pred[y_pred<0.5] = 0

from sklearn.metrics import classification_report, confusion_matrix

"""**Classification Report**"""

print(classification_report(y_test,y_pred,target_names = ['Normal','Pnuemonia']))

"""**Confusion Matrix**"""

plt.figure(figsize=(10,8))
sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt='.3g',xticklabels=['Normal','Pnuemonia'],
            yticklabels=['Normal','Pnuemonia'],cmap='Blues')
plt.show()

"""# 6. Concluing through model experiments

**Model Diagnosis**
* Indentifying images that are misclassified for the test data
"""

filenames = testdata.filenames
data = pd.DataFrame()
data['filename'] = filenames
data['actual_class'] = y_test
data['predicted_class'] = y_pred
data['predicted_prob'] = y_pred_probs

misclassification = data[data['actual_class']!=data['predicted_class']]

misclassification[(misclassification['actual_class']==0) & (misclassification['predicted_prob']>0.9)]

misclassification[(misclassification['actual_class']==1) & (misclassification['predicted_prob']<0.5)]

"""**Displaying Missclassified Image**"""

img = load_img('/tmp/chest_xray/test/NORMAL/IM-0022-0001.jpeg',target_size=(1000,1000))
img = img_to_array(img)/255.
plt.figure(figsize = (10 , 10))
plt.imshow(img)

"""**Feature Map Visualizations**"""

img = np.expand_dims(img,axis=0)
img.shape

model.layers

feature_extractor = Model(model.inputs,model.layers[1].output)

features = feature_extractor.predict(valdata)
features.shape

"""**Gives You the top 15 feratures learnt from the last second layer.**"""

plt.figure(figsize = (20 , 20))
for i in range(15):
    plt.subplot(5 , 5, i+1)
    plt.subplots_adjust(hspace = 0.3 , wspace = 0.3)
    plt.imshow(features[0,:,:,i])
    plt.axis('off')