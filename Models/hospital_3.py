# -*- coding: utf-8 -*-
"""Hospital_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Spey5K2HwvHUpZgHCD390jRIKfOzpkyM

# 1.  Importing necessary Modules.
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams['figure.figsize']=(20,20)

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

zip_ref = zipfile.ZipFile('/content/drive/MyDrive/H3_data.zip', 'r') #Opens the zip file in read mode
zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder
zip_ref.close()

"""# 2. Looking at our images.

**Pnuemonia X Ray Image**
"""

from glob import glob #retriving an array of files in directories
path_train = "/tmp/train"
img = glob(path_train+"/PNEUMONIA/*.jpeg")
# print(img)
img = np.asarray(plt.imread(img[0]))
plt.figure(figsize = (5 , 5))
plt.imshow(img)

"""**Normal X Ray Image**"""

img = glob(path_train+"/NORMAL/*.jpeg")
img = np.asarray(plt.imread(img[0]))
plt.figure(figsize = (5 , 5))
plt.imshow(img)

"""**Importing libraries that are used for creating a convolution neural network model.**"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array

from tensorflow.keras.models import Sequential,Model

from tensorflow.keras.layers import Conv2D,Dense,Flatten,Input,MaxPooling2D,Dropout,BatchNormalization

"""**Using Image Data Generator we load the images to our model.**"""

img_size=150
batch_size=25

traingen=ImageDataGenerator(rescale=1/255.,
                           rotation_range=50,
                        brightness_range=[0.2,1.2],
                           width_shift_range=0.1,
                           height_shift_range=0.1,
                           horizontal_flip=True)


testgen=ImageDataGenerator(rescale=1/255.)

valgen=ImageDataGenerator(rescale=1/255.)

"""* **traindata** : stores all the images from the train directroy
* **testdata** : stores all the images from the test directory
* **valdata** : stores all the images from the val directory
"""

traindata=traingen.flow_from_directory('/tmp/train',
                                       target_size=(img_size,img_size)
                                       ,batch_size=batch_size,
                                       shuffle=True,class_mode='binary'
                                      ,color_mode='grayscale')

testdata=testgen.flow_from_directory('/tmp/test',
                                    shuffle=False,batch_size=batch_size,
                                    target_size=(img_size,img_size),
                                    class_mode="binary",color_mode='grayscale')
valdata=valgen.flow_from_directory('/tmp/val',
                                    shuffle=False,batch_size=batch_size,
                                    target_size=(img_size,img_size),
                                    class_mode="binary",color_mode='grayscale')

# traindata.__dict__
# testdata.__dict__
# valdata.__dict__

"""**Printing 15 images from the train data**"""

import scipy
labels=['Normal','Pnuemonia']
samples=traindata.__next__()

images=samples[0]
target=samples[1]

for i in range(15):
    plt.subplot(5,5,i+1)
    plt.subplots_adjust(hspace=0.3,wspace=.3)
    plt.imshow(images[i])
    plt.title(f"Class: {labels[int(target[i])]}")
    plt.axis('off')

"""**Meta Data**

0 here is a label Normal and 1 is for Pnuemonia
"""

df=pd.DataFrame(traindata.classes)
df.value_counts()

df=pd.DataFrame(valdata.classes)
df.value_counts()

df=pd.DataFrame(testdata.classes)
df.value_counts()

"""# 4. Creating our Counvolution Neural Network."""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
model=Sequential()
model.add(Conv2D(32,(2,2),input_shape=(img_size,img_size,1),activation="relu",padding='same',strides=1))
model.add(MaxPooling2D())
model.add(Conv2D(64,(2,2),strides=2,activation="relu",padding='same'))
model.add(MaxPooling2D())
model.add(Conv2D(128,(2,2),strides=1,activation="relu",padding='same'))
model.add(MaxPooling2D())
model.add(Flatten())
model.add(Dense(128,activation="relu"))
model.add(Dense(1,activation='sigmoid'))

model.summary()

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
filepath= "H3.h5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)

"""# 5. Training Our Convolution Neural Network."""

history=model.fit(traindata,validation_data=valdata,epochs=10,callbacks=[checkpoint])

"""**Plotting Lossand and Accuracy Curves from training and validation**"""

plt.figure(figsize=(20,8))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

plt.figure(figsize=(20,8))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

"""## Checking the model prediction for the val data"""

y_test = valdata.classes
y_pred = model.predict(valdata)
y_pred_probs = y_pred.copy()
y_pred[y_pred>0.5] = 1
y_pred[y_pred<0.5] = 0

from sklearn.metrics import classification_report, confusion_matrix

"""**Classification Report**"""

print(classification_report(y_test,y_pred,target_names = ['Normal','Pnuemonia']))

"""**Confusion Matrix**"""

plt.figure(figsize=(10,8))
sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt='.3g',xticklabels=['Normal','Pnuemonia'],
            yticklabels=['Normal','Pnuemonia'],cmap='Blues')
plt.show()

"""## Checking the model prediction for the test data"""

y_test = testdata.classes
y_pred = model.predict(testdata)
y_pred_probs = y_pred.copy()

y_pred[y_pred>0.5] = 1
y_pred[y_pred<0.5] = 0

from sklearn.metrics import classification_report, confusion_matrix

"""**Classification Report**"""

print(classification_report(y_test,y_pred,target_names = ['Normal','Pnuemonia']))

"""**Confusion Matrix**"""

plt.figure(figsize=(10,8))
sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt='.3g',xticklabels=['Normal','Pnuemonia'],
            yticklabels=['Normal','Pnuemonia'],cmap='Blues')
plt.show()

"""# 6. Concluing through model experiments

**Model Diagnosis**
* Indentifying images that are misclassified for the test data
"""

filenames = testdata.filenames
data = pd.DataFrame()
data['filename'] = filenames
data['actual_class'] = y_test
data['predicted_class'] = y_pred
data['predicted_prob'] = y_pred_probs

misclassification = data[data['actual_class']!=data['predicted_class']]

"""**Which images are getting misclassified and with how much probability ?**
* all the files listed below are being misclassified
"""

misclassification[(misclassification['actual_class']==0) & (misclassification['predicted_prob']>0.9)]

misclassification[(misclassification['actual_class']==1) & (misclassification['predicted_prob']<0.5)]

"""**Displaying Missclassified Image**"""

img = load_img('/tmp/chest_xray/test/NORMAL/IM-0022-0001.jpeg',target_size=(1000,1000))
img = img_to_array(img)/255.
plt.figure(figsize = (10 , 10))
plt.imshow(img)

"""**Feature Map Visualizations**"""

img = np.expand_dims(img,axis=0)
img.shape

model.layers

feature_extractor = Model(model.inputs,model.layers[1].output)

features = feature_extractor.predict(valdata)
features.shape

"""**Gives You the top 15 feratures learnt from the last second layer.**"""

plt.figure(figsize = (20 , 20))
for i in range(15):
    plt.subplot(5 , 5, i+1)
    plt.subplots_adjust(hspace = 0.3 , wspace = 0.3)
    plt.imshow(features[0,:,:,i])
    plt.axis('off')



import h5py
import numpy as np
def isGroup(obj):
    if isinstance(obj,h5py.Group):
        return True
    return False

def isDataset(obj):
    if isinstance(obj,h5py.Dataset):
        return True
    return False

def getDatasetFromGroup(datasets,obj):
    if isGroup(obj):
        for key in obj:
            x = obj[key]
            getDatasetFromGroup(datasets,x)
    else:
        datasets.append(obj)

def getWeightsForLayer(layerName, filename):
   weights = []
   with h5py.File(filename, mode='r') as f:
       for key in f:
           if layerName in key:
              obj = f[key]
              datasets = []
              getDatasetFromGroup(datasets,obj)

              for dataset in datasets:
                  w = np.array(dataset)
                  weights.append(w)
   return weights
           #print(key, f[key])
           #o = f[key]
           #for key1 in o:
               #print(key1,o[key1])
               #r = o[key1]
               #for key2 in r:
                   #print(key2,r[key2])
weights = getWeightsForLayer("conv2d_6","/content/model_cnn.h5")
for w in weights:
    print(w.shape)
print(weights)

#Load the .h5 file
model = tf.keras.models.load_model('H3.h5')

#Extract the model parameters
weights = model.get_weights()

#Print the extracted weights
for i in range(len(weights)):
    if i == (len(weights)-1) :
        # print('Layer {}:'.format(i))
        # print(weights[i])
        w1= weights[i][0]
        print(w1)


weights1 = weights
weights1[9][0]=0.02629807591
model.set_weights(weights1)
print(model.get_weights()[9][0])

y_test = testdata.classes
y_pred = model.predict(testdata)
y_pred_probs = y_pred.copy()

y_pred[y_pred>0.5] = 1
y_pred[y_pred<0.5] = 0

from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(y_test,y_pred,target_names = ['Normal','Pnuemonia']))